# ğŸ¤– LLM Role in the Web Crawler

## The Complete Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 1: CRAWLING                            â”‚
â”‚                    (NO LLM INVOLVED)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. WebCrawler (crawler.py)                                    â”‚
â”‚     â””â”€> requests library                                       â”‚
â”‚         â””â”€> Fetch HTML from URLs                               â”‚
â”‚         â””â”€> Follow links                                       â”‚
â”‚         â””â”€> Apply rate limiting                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PHASE 2: PROCESSING                           â”‚
â”‚              (LLM USED IN STEP 2.2 ONLY)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  2.1 ContentProcessor (content_processor.py)                   â”‚
â”‚      â””â”€> BeautifulSoup: Parse HTML                             â”‚
â”‚      â””â”€> Rule-based: Remove boilerplate                        â”‚
â”‚      â””â”€> BeautifulSoup: Extract main content                   â”‚
â”‚      â””â”€> markdownify: Convert to Markdown                      â”‚
â”‚      â””â”€> Extract: Links, word count, checksum                  â”‚
â”‚      â””â”€> slugify: Generate URL-safe slug                       â”‚
â”‚                                                                 â”‚
â”‚         â¬‡ Extracted content passed to LLM                       â”‚
â”‚                                                                 â”‚
â”‚  2.2 LLMNormalizer (llm_normalizer.py) â­ LLM HERE             â”‚
â”‚      â””â”€> LangChain + ChatOllama                                â”‚
â”‚          â””â”€> Model: llama3.1:8b                                â”‚
â”‚          â””â”€> Temperature: 0.3 (deterministic)                  â”‚
â”‚          â””â”€> Two operations:                                   â”‚
â”‚              â”œâ”€> improve_title()                               â”‚
â”‚              â”‚   Input:  "Example Domain"                      â”‚
â”‚              â”‚   Output: "Use as Example Domain in Documents"  â”‚
â”‚              â”‚                                                  â”‚
â”‚              â””â”€> extract_tags()                                â”‚
â”‚                  Input:  Title + Content (first 1000 chars)    â”‚
â”‚                  Output: [example, domain, illustration, ...]  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PHASE 3: OUTPUT                               â”‚
â”‚                   (NO LLM INVOLVED)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  3. ObsidianWriter (obsidian_writer.py)                        â”‚
â”‚     â””â”€> Generate YAML frontmatter                              â”‚
â”‚     â””â”€> Write .md files to vault                               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Processing Breakdown by Component

| Component | Tool/Library | LLM Used? | Purpose |
|-----------|-------------|-----------|---------|
| **URL Fetching** | requests | âŒ No | Download HTML |
| **HTML Parsing** | BeautifulSoup | âŒ No | Parse DOM structure |
| **Boilerplate Removal** | BeautifulSoup + CSS selectors | âŒ No | Remove nav, footer, ads |
| **Content Extraction** | BeautifulSoup | âŒ No | Get main text content |
| **Markdown Conversion** | markdownify | âŒ No | HTML â†’ Markdown |
| **Link Discovery** | BeautifulSoup | âŒ No | Extract <a> tags |
| **Title Improvement** | LangChain + Llama 3.1 | âœ… **YES** | Enhance title clarity |
| **Tag Extraction** | LangChain + Llama 3.1 | âœ… **YES** | Generate relevant tags |
| **Slug Generation** | python-slugify + hashlib | âŒ No | Create URL-safe filename |
| **File Writing** | Python pathlib | âŒ No | Write .md files |

## ğŸ¯ Specific LLM Usage

### Location: `llm_normalizer.py` (lines 27-122)

**Function 1: `improve_title()`**
```python
# Input received from ContentProcessor
title = "Example Domain"  # Extracted by BeautifulSoup
content_preview = "This domain is for use in..."  # First 500 chars

# LLM Prompt (via LangChain)
prompt = """
You are a helpful assistant that improves web page titles.
Create a clear, concise title (max 80 chars).
Return ONLY a JSON object: {"title": "improved title"}
"""

# LLM processes with Llama 3.1:8b
response = llm.invoke(prompt)

# Output
improved_title = "Use as Example Domain in Documents"
```

**Function 2: `extract_tags()`**
```python
# Input received from ContentProcessor
title = "Use as Example Domain in Documents"
content_sample = content[:1000]  # First 1000 chars

# LLM Prompt (via LangChain)
prompt = """
You are a helpful assistant that extracts relevant tags.
Return ONLY a JSON object: {"tags": ["tag1", "tag2", "tag3"]}
Include 3-7 relevant, lowercase tags.
"""

# LLM processes with Llama 3.1:8b
response = llm.invoke(prompt)

# Output
tags = ["example", "domain", "illustration", "documentation", "reference"]
```

## â±ï¸ Performance Impact

From our test results:

- **Without LLM** (`--skip-llm`): ~0.5 seconds per page
- **With LLM**: ~4-5 seconds per page
  - Title improvement: ~2 seconds
  - Tag extraction: ~2 seconds
  - **10x slower but adds semantic value**

## ğŸ”„ Can You Run Without LLM?

**Yes!** Use the `--skip-llm` flag:

```bash
python main.py --skip-llm
```

When skipped:
- âœ… Crawling still works
- âœ… Content extraction still works
- âœ… Markdown conversion still works
- âœ… Obsidian files still created
- âŒ Title remains as-extracted from HTML
- âŒ Tags default to `["crawled"]`

## ğŸ“ Summary

**The LLM (Llama 3.1 via LangChain) is used as a POST-PROCESSOR:**

1. âŒ **NOT** for crawling decisions
2. âŒ **NOT** for HTML parsing
3. âŒ **NOT** for content extraction
4. âœ… **ONLY** for semantic enhancement:
   - Making titles more descriptive
   - Generating contextual tags

**It's an optional enhancement layer** that adds intelligence to the metadata, but the core crawler works perfectly without it using traditional web scraping techniques (BeautifulSoup, markdownify, etc.).

---

**Key Insight:** The crawler uses a **hybrid approach**:
- Traditional libraries for **reliable, fast parsing**
- LLM for **semantic understanding and enhancement**
