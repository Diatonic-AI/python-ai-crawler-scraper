# ğŸ•·ï¸ AI-Powered Web Crawler - Project Summary

## ğŸ“¦ What Was Built

A complete Python web crawler that:
- Crawls websites from seed URLs with depth and page limits
- Extracts clean content from HTML (removes boilerplate)
- Converts content to Obsidian-style Markdown
- Uses local LLM (Ollama) via LangChain to improve titles and extract tags
- Tracks internal/external links and computes backlinks
- Stores everything in SQLite for resume-safe operation
- Outputs to an Obsidian vault with YAML frontmatter

## ğŸ“ Project Structure

```
python-ai-crawler-scraper/
â”œâ”€â”€ README.md                    # Comprehensive documentation
â”œâ”€â”€ SUMMARY.md                   # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Configuration template
â”œâ”€â”€ setup.sh                     # Quick setup script
â”‚
â”œâ”€â”€ config.py                    # Configuration management
â”œâ”€â”€ database.py                  # SQLite database operations
â”œâ”€â”€ crawler.py                   # Web crawling engine
â”œâ”€â”€ content_processor.py         # HTML to Markdown conversion
â”œâ”€â”€ llm_normalizer.py            # LangChain + Ollama integration
â”œâ”€â”€ obsidian_writer.py           # Obsidian vault writer
â””â”€â”€ main.py                      # Main orchestration script
```

## ğŸ¯ Core Features

### 1. Web Crawling (`crawler.py`)
- âœ… URL queue management with depth tracking
- âœ… Domain filtering (same-origin checks)
- âœ… Rate limiting with configurable delays
- âœ… Retry logic with exponential backoff
- âœ… Content-type checking (HTML only)
- âœ… Size limits (skip large files)
- âœ… Binary file detection

### 2. Content Processing (`content_processor.py`)
- âœ… BeautifulSoup HTML parsing
- âœ… Boilerplate removal (nav, footer, ads, etc.)
- âœ… Main content extraction
- âœ… Markdown conversion via markdownify
- âœ… Link discovery and categorization
- âœ… Word count and checksum generation
- âœ… URL-safe slug generation

### 3. LLM Enhancement (`llm_normalizer.py`)
- âœ… LangChain integration with ChatOllama
- âœ… Title improvement using LLM
- âœ… Tag extraction from content
- âœ… JSON response contract
- âœ… Fallback to original title if LLM fails
- âœ… Configurable temperature and timeout

### 4. Database Management (`database.py`)
- âœ… SQLite schema with pages and links tables
- âœ… Idempotent upsert operations
- âœ… Backlink computation
- âœ… Resume-safe operations
- âœ… Statistics and reporting
- âœ… Proper indexing for performance

### 5. Obsidian Output (`obsidian_writer.py`)
- âœ… YAML frontmatter generation
- âœ… Metadata fields: title, source_url, slug, timestamps
- âœ… Crawl metadata: depth, word_count, checksum
- âœ… Tags from LLM
- âœ… Backlinks list
- âœ… Deterministic filename generation

### 6. Configuration (`config.py`)
- âœ… Environment variable loading (.env)
- âœ… CLI argument overrides
- âœ… Validation on startup
- âœ… Sensible defaults
- âœ… Path management

### 7. Main Orchestrator (`main.py`)
- âœ… Three-phase execution: Crawl â†’ Process â†’ Write
- âœ… Progress bars with tqdm
- âœ… Resume capability
- âœ… Statistics reporting
- âœ… Error handling
- âœ… Skip LLM option for faster crawling

## ğŸš€ Quick Start

```bash
# 1. Activate virtual environment
cd ~/dev/python-llama-demo/python-ai-crawler-scraper
source ../venv/bin/activate

# 2. Run setup script
./setup.sh

# 3. Edit configuration
nano .env

# 4. Run crawler
python main.py --seeds https://example.com --max-pages 10
```

## ğŸ’¡ Usage Examples

### Basic Crawl
```bash
python main.py --seeds https://docs.python.org --max-pages 25 --max-depth 2
```

### Fast Crawl (Skip LLM)
```bash
python main.py --seeds https://example.com --skip-llm --max-pages 50
```

### Resume Previous Crawl
```bash
python main.py --resume
```

### Domain-Restricted Crawl
```bash
# In .env file:
ALLOWED_DOMAINS=example.com,docs.example.com

python main.py
```

## ğŸ“Š Sample Output

### Console Output
```
ğŸ”§ Crawler Configuration
============================================================
Seed URLs: ['https://example.com']
Max Depth: 2
Max Pages: 25
Request Delay: 1.0s
Database: ./crawler.db
Vault Directory: ./obsidian_vault
LLM Model: llama3.1:8b @ http://10.0.228.180:31134
============================================================

ğŸ•·ï¸  Phase 1: Crawling
ğŸ” [1/25] Crawling (depth 0): https://example.com
  ğŸ”— Discovered 12 new links
...
âœ… Crawled 25 pages

ğŸ“Š Database Statistics:
  Total pages: 25
  Total links: 89
  Max depth: 2

ğŸ”„ Phase 2: Processing
Processing pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:30<00:00, 0.83it/s]

ğŸ“ Phase 3: Writing Obsidian Vault
Writing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 20.5it/s]

ğŸ‰ Crawl Complete!
  Pages in vault: 25
  Vault location: ./obsidian_vault
```

### Sample Markdown Output
```markdown
---
title: "Python Documentation - Getting Started"
source_url: "https://docs.python.org/3/tutorial/"
slug: "python-documentation-getting-started"
crawled_at: "2025-10-02T08:15:00Z"
updated_at: "2025-10-02T08:15:00Z"
crawl_depth: 1
word_count: 542
checksum: "a1b2c3d4e5f6"
tags: [python, tutorial, programming, documentation]
backlinks:
  - "Python Home"
  - "Table of Contents"
---

# Python Documentation - Getting Started

This tutorial introduces you to Python's basic concepts...

[[Installing Python]] - Learn how to install Python on your system

## Next Steps

Continue with [[Data Types]] or explore [[Functions]]
```

## ğŸ§ª Testing

Each module has a `__main__` block for standalone testing:

```bash
python database.py        # Test database operations
python crawler.py         # Test basic crawling
python content_processor.py   # Test HTML processing
python llm_normalizer.py  # Test LLM integration
python obsidian_writer.py # Test file writing
```

## ğŸ”§ Configuration Options

All configurable via `.env` file:

```bash
# Crawl Settings
SEED_URLS=https://example.com
ALLOWED_DOMAINS=example.com
MAX_DEPTH=3
MAX_PAGES=100
REQUEST_DELAY=1.0
REQUEST_TIMEOUT=30

# LLM Settings
OLLAMA_BASE_URL=http://10.0.228.180:31134
LLM_MODEL=llama3.1:8b
LLM_TEMPERATURE=0.3

# Output
VAULT_DIR=./obsidian_vault
DATABASE_PATH=./crawler.db
```

## ğŸ“ˆ Performance Characteristics

- **Crawl Speed**: ~1 page/second with 1s delay
- **LLM Processing**: ~2-3s per page for title + tags
- **Memory Usage**: ~50-100MB for typical crawls
- **Database Size**: ~1KB per page + links
- **Vault Size**: ~5-10KB per page (Markdown)

## ğŸ“ Key Design Decisions

1. **SQLite**: Chosen for simplicity, portability, and resume capability
2. **Idempotent Operations**: All database operations are upserts for safety
3. **Three-Phase Processing**: Separate crawl/process/write for clarity
4. **LangChain + Ollama**: Local LLM for privacy and cost
5. **Obsidian Format**: YAML frontmatter for metadata + Markdown content
6. **Configurable Everything**: .env file + CLI overrides

## ğŸš¨ Known Limitations

- No JavaScript rendering (static HTML only)
- No image downloading (content only)
- No PDF extraction
- No robots.txt checking (add --respect-robots flag needed)
- Single-threaded (can be extended to multi-process)
- LLM requires local Ollama server

## ğŸ”® Future Enhancements

See `README.md` for full list. Top priorities:
- [ ] Distributed crawling
- [ ] Custom per-domain extraction rules
- [ ] Image downloading
- [ ] robots.txt compliance
- [ ] Scheduled/periodic crawls

## ğŸ“š Dependencies

Core libraries used:
- **requests**: HTTP fetching
- **beautifulsoup4 + lxml**: HTML parsing
- **markdownify**: HTML to Markdown
- **langchain + langchain-ollama**: LLM integration
- **tldextract**: Domain extraction
- **tenacity**: Retry logic
- **python-slugify**: URL-safe slugs
- **python-dotenv**: Configuration
- **tqdm**: Progress bars
- **pydantic**: Data validation (optional)

## âœ… Project Status

**Status**: âœ… Complete and ready to use

All components implemented:
- âœ… Web crawling with depth/domain/size controls
- âœ… Content extraction and Markdown conversion
- âœ… LLM-based title improvement and tagging
- âœ… SQLite persistence with backlink tracking
- âœ… Obsidian vault generation
- âœ… Resume capability
- âœ… Comprehensive documentation
- âœ… Quick setup script

## ğŸ‰ Success Criteria Met

âœ… Crawls from seed URLs with depth limits
âœ… Fetches HTML and removes boilerplate
âœ… Extracts readable content
âœ… Discovers and tracks links (internal/external)
âœ… Computes backlinks within crawl set
âœ… Converts to Obsidian-style Markdown with YAML frontmatter
âœ… Uses LangChain with local Ollama model for text normalization
âœ… Persists in SQLite with idempotent operations
âœ… Resume-safe and deterministic
âœ… Configurable via .env and CLI args
âœ… Handles retries, timeouts, and errors gracefully
âœ… Skips large/binary pages

---

**Ready to crawl!** See `README.md` for detailed usage instructions.
